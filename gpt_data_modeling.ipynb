{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Dataset into Chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Read Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import GPT2Config\n",
        "\n",
        "# Load the GPT2 configuration\n",
        "config = GPT2Config()\n",
        "\n",
        "# Load the tokenizer from the local directory)\n",
        "tokenizer = ByteLevelBPETokenizer(\n",
        "    \"./tokenizer/vocab.json\",\n",
        "    \"./tokenizer/merges.txt\",\n",
        "    add_prefix_space=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Carolina Wik Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nando/miniconda3/envs/torch/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for carolina-c4ai/corpus-carolina contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/carolina-c4ai/corpus-carolina\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"carolina-c4ai/corpus-carolina\", taxonomy=\"wik\")['corpus']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Shuffle the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = dataset.shuffle(seed=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create tokenized dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def generate_chunk(dataset, target_size):\n",
        "    current_chunk = torch.empty((0,))\n",
        "    for entry in dataset['text']:\n",
        "        # Tokenize entry\n",
        "        entry = tokenizer.encode(entry, add_special_tokens=False).ids\n",
        "        entry = torch.asarray(entry, dtype=torch.long)\n",
        "\n",
        "        # Update chunk\n",
        "        current_chunk = torch.cat([current_chunk, entry])\n",
        "\n",
        "        # Yield chunks\n",
        "        while current_chunk.shape[0] > target_size:\n",
        "            yield current_chunk[:target_size]\n",
        "            current_chunk = current_chunk[target_size:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "550312it [1:42:30, 89.47it/s]\n"
          ]
        }
      ],
      "source": [
        "import zarr\n",
        "\n",
        "CHUNK_SIZE = config.n_positions - 2\n",
        "\n",
        "zarr_array = zarr.open(\n",
        "    \"dataset.zarr\",\n",
        "    mode='w',\n",
        "    shape=(0, CHUNK_SIZE),\n",
        "    dtype='long',\n",
        "    chunks=(1, CHUNK_SIZE)\n",
        ")\n",
        "\n",
        "for chunk in tqdm(generate_chunk(dataset, CHUNK_SIZE)):\n",
        "    current_shape = zarr_array.shape\n",
        "    new_shape = list(current_shape)\n",
        "    new_shape[0] += 1\n",
        "    zarr_array.resize(tuple(new_shape))\n",
        "    zarr_array[-1, :] = chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import zarr\n",
        "old_zarr = zarr.open(\"dataset.zarr\", \"r\")\n",
        "CHUNK_SIZE = config.n_positions - 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_zarr = zarr.open(\"dataset_copy.zarr\", \"w\", shape=old_zarr.shape, dtype=\"int\", chunk=(2048, CHUNK_SIZE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_zarr[:] = old_zarr[:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.1.undefined"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
