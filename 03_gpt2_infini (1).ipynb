{"cells":[{"cell_type":"markdown","metadata":{"id":"y8DQeHelDd3v"},"source":["# Training GPT-2 Model with InfiniAttention Module"]},{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718801595449,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"UF_J-n4_Rayu"},"outputs":[],"source":["# zarr_file_path = './dataset_copy.zarr'\n","tokenizer_path = '../tokenizer/'\n","# config_path = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/configs/config.json'"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":3067,"status":"ok","timestamp":1718801598513,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"x-wprKVYDd3y"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import GPT2Config, GPT2LMHeadModel\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n","from typing import Optional, Tuple, Union"]},{"cell_type":"markdown","metadata":{"id":"oojHktmSDd3z"},"source":["### Standard GPT2LMHeadModel structure"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718801598514,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"0WKZ8taQDd30"},"outputs":[],"source":["config = GPT2Config()\n","# model = GPT2LMHeadModel(config)"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718801598514,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"HHbv9CxSDd32"},"outputs":[],"source":["import torch.nn.functional as F\n","from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, Conv1D\n","\n","\n","class InfiniAttentionGPT2(GPT2Attention):\n","    def __init__(\n","        self,\n","        config,\n","        is_cross_attention=False,\n","        layer_idx=None,\n","        n_segments=16,\n","        is_causal: Optional[bool] = True,\n","        update: Optional[str] = \"linear\",\n","    ):\n","        \"\"\"\n","        Initialize the InfiniAttentionGPT2 module.\n","\n","        Args:\n","            config: GPT2Config object containing configuration parameters.\n","            is_cross_attention (bool): Flag to indicate if this is cross attention.\n","            layer_idx (int, optional): Index of the layer.\n","            n_segments (int): Number of segments for memory processing.\n","            is_causal (bool, optional): Flag to indicate if attention is causal.\n","            update (str, optional): Update strategy for memory, either 'linear' or another strategy.\n","        \"\"\"\n","        super().__init__(config, is_cross_attention, layer_idx)\n","\n","        # Initializing memory state for compressive memory\n","        self.d_head = config.hidden_size // config.num_attention_heads\n","        self.n_head = config.num_attention_heads\n","\n","        # Initialize the beta parameter for combining A_mem and A_dot\n","        self.beta = nn.Parameter(torch.zeros(1), requires_grad=True)\n","\n","        self.elu = nn.ELU()\n","\n","        # Sequence length\n","        self.seq_len = config.n_positions\n","\n","        self.is_causal = is_causal\n","        self.register_buffer(\n","            \"causal\",\n","            torch.tril(\n","                torch.ones(self.seq_len // n_segments, self.seq_len // n_segments)\n","            ),\n","        )\n","\n","        # Segment size\n","        self.n_segments = n_segments\n","        self.segment_size = self.seq_len // n_segments\n","\n","        # Update strategy\n","        self.update = update\n","\n","    def _retrieve_from_memory(self, query_states):\n","        # Retrieve context from compressive memory using linear attention (Eq. 3)\n","        if self.memory is None:\n","            return torch.zeros_like(query_states)\n","        query_states = F.elu(query_states) + 1  # ELU activation\n","        memory_output = torch.matmul(query_states, self.memory) / self.norm_term\n","        return memory_output\n","\n","    def _update_memory(self, key_states, value_states):\n","        # Update compressive memory with new key-value states (Eq. 4)\n","        key_states = F.elu(key_states) + 1  # ELU activation\n","        if self.memory is not None:\n","            self.memory = self.memory + torch.matmul(\n","                key_states.transpose(-2, -1), value_states\n","            )\n","        else:\n","            self.memory = torch.matmul(key_states.transpose(-2, -1), value_states)\n","        if self.norm_term is not None:\n","            self.norm_term = self.norm_term + torch.unsqueeze(\n","                key_states.sum(dim=-2), -2\n","            )\n","        else:\n","            self.norm_term = torch.unsqueeze(key_states.sum(dim=-2), -2)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.FloatTensor,\n","        layer_past: Optional[Tuple[torch.Tensor]] = None,\n","        attention_mask: Optional[torch.FloatTensor] = None,\n","        head_mask: Optional[torch.FloatTensor] = None,\n","        use_cache: Optional[bool] = False,\n","        output_attentions: Optional[bool] = False,\n","        debug: Optional[bool] = False,\n","    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n","\n","        self.norm_term = None\n","        self.memory = None\n","\n","        batch_size, _, _ = hidden_states.size()\n","\n","        device = hidden_states.device\n","\n","        qkv = self.c_attn(hidden_states)\n","        query, key, value = qkv.split(self.split_size, dim=2)\n","\n","        # segments = torch.tensor_split(\n","        #     hidden_states,\n","        #     list(range(self.segment_size, hidden_states.size(1), self.segment_size)),\n","        #     dim=1,\n","        # )\n","\n","        segments_q = torch.tensor_split(\n","            query,\n","            list(range(self.segment_size, query.size(1), self.segment_size)),\n","            dim=1,\n","        )\n","\n","        segments_k = torch.tensor_split(\n","            key,\n","            list(range(self.segment_size, key.size(1), self.segment_size)),\n","            dim=1,\n","        )\n","\n","        segments_v = torch.tensor_split(\n","            value,\n","            list(range(self.segment_size, value.size(1), self.segment_size)),\n","            dim=1,\n","        )\n","\n","        final_outputs = []\n","        final_k = []\n","        final_attn = []\n","\n","        # print(f\"{hidden_states.shape=}\")\n","        # print(f\"{self.c_attn(hidden_states).shape=}\")\n","        # print(f\"{segments_q[0].shape=}\")\n","\n","        for i, segment in enumerate(segments_q):\n","            # print(f\"{segment.shape=}\")\n","            # qkv_segment = self.c_attn(segment)\n","            # query, key, value = qkv_segment.split(self.split_size, dim=2)\n","\n","            query = segments_q[i]\n","            key = segments_k[i]\n","            value = segments_v[i]\n","\n","            query = self._split_heads(\n","                query, num_heads=self.n_head, attn_head_size=self.d_head\n","            )\n","            key = self._split_heads(\n","                key, num_heads=self.n_head, attn_head_size=self.d_head\n","            )\n","            final_k.append(key)\n","            value = self._split_heads(\n","                value, num_heads=self.n_head, attn_head_size=self.d_head\n","            )\n","\n","            bsz, q_len, _ = segment.size()\n","\n","            # print(f\"{query.shape=}\")\n","            memory_output = self._retrieve_from_memory(query)\n","            self._update_memory(key, value)\n","\n","            # print(f\"{attention_mask.shape=}\")\n","            if attention_mask is not None:\n","                attention_mask_segment = attention_mask[:, :, :, : self.segment_size]\n","            # print(f\"{attention_mask_segment.shape=}\")\n","            attn_outputs = self._attn(\n","                query, key, value, attention_mask_segment, head_mask\n","            )\n","            a_dot = attn_outputs[0]\n","            final_attn.append(attn_outputs[1])\n","\n","            # attn_output = torch.nn.functional.scaled_dot_product_attention(\n","            #     query_states,\n","            #     key_states,\n","            #     value_states,\n","            #     attn_mask=causal_mask,\n","            #     dropout_p=self.attention_dropout if self.training else 0.0,\n","            # )\n","\n","            combined_output = (\n","                F.sigmoid(self.beta) * memory_output\n","                + (1 - F.sigmoid(self.beta)) * a_dot\n","            )\n","\n","            combined_output = self._merge_heads(\n","                combined_output, self.n_head, self.d_head\n","            )\n","            combined_output = self.c_proj(combined_output)\n","            combined_output = self.resid_dropout(combined_output)\n","\n","            final_outputs.append(combined_output)\n","\n","        final_outputs = torch.cat(final_outputs, dim=1)\n","        final_k = torch.cat(final_k, dim=1)\n","        final_attn = torch.cat(final_attn, dim=1)\n","        outputs = (final_outputs, final_k)\n","\n","        if output_attentions:\n","            outputs = outputs + (final_attn,)\n","\n","        return outputs"]},{"cell_type":"markdown","metadata":{"id":"6Ig48AZuDd33"},"source":["### Training Model"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718801598514,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"JJ3Jo6efDd33"},"outputs":[],"source":["model_type = \"gpt2-infini\"  # \"gpt2\" or \"gpt2-infini\""]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718801598514,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"97qnjTscDd34"},"outputs":[],"source":["# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cuda:0\")"]},{"cell_type":"markdown","metadata":{"id":"2E7fzdYItj6U"},"source":["### Trainer"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":4464,"status":"ok","timestamp":1718801602975,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"ula9E0Slk0CA"},"outputs":[],"source":["# gpt-2 original\n","# model = GPT2LMHeadModel(config).to(device)\n","\n","# gpt-2 infini\n","model = GPT2LMHeadModel(config)\n","\n","for i, layer in enumerate(model.transformer.h):\n","    model.transformer.h[i].attn = InfiniAttentionGPT2(\n","        config, layer_idx=i, n_segments=128\n","    )\n","\n","model = model.to(device)"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718801602975,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"j9NJh8KzecKr","outputId":"0b4944d1-9f1f-4431-e11d-a5962b0d65a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["The model has 124,439,820 trainable parameters\n"]}],"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(model):,} trainable parameters')"]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":764,"status":"ok","timestamp":1718801603737,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"l1cpVhS4fQn7"},"outputs":[],"source":["from datasets import Dataset\n","import torch\n","\n","path_dataset = '../datasets/split_10/'\n","\n","train_dataset = Dataset.load_from_disk(path_dataset + \"train_dataset\")\n","test_dataset = Dataset.load_from_disk(path_dataset + \"test_dataset\")\n","\n","# 50% train data and 5% of test data of 50% train data.\n","train_dataset = train_dataset.select(range(int(len(train_dataset))))\n","# train_dataset = train_dataset.select(range(160))\n","test_dataset = test_dataset.select(range(4 * 16)) # 6 * 16"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718801603738,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"QA16JUBtpqnc","outputId":"ee4985eb-7892-48f5-e500-b7c92c340cc2"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'attention_mask'],\n","    num_rows: 53930\n","})"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["train_dataset"]},{"cell_type":"code","execution_count":60,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718801603738,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"AXcDE55aoJT0","outputId":"e815ee60-6da2-4c24-f094-2f0b4cf9b491"},"outputs":[{"data":{"text/plain":["Dataset({\n","    features: ['input_ids', 'attention_mask'],\n","    num_rows: 64\n","})"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["test_dataset"]},{"cell_type":"code","execution_count":61,"metadata":{"executionInfo":{"elapsed":2525,"status":"ok","timestamp":1718801606261,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"Ez6BSiCCtki1"},"outputs":[],"source":["# train with trainer\n","from transformers import GPT2Tokenizer\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import Trainer, TrainingArguments\n","from tokenizers import ByteLevelBPETokenizer\n","import numpy as np\n","from datasets import Dataset\n","\n","output_dir = '../models/output_dir'\n","logging_dir = '../models/logs'\n","model_save_dir = '../models/'\n","\n","# batch_size = 16\n","# num_epochs = 1\n","\n","# num_steps = len(train_dataset) * num_epochs // batch_size\n","\n","training_args = TrainingArguments(\n","    learning_rate=1e-4,\n","    output_dir=output_dir,\n","    num_train_epochs=5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    logging_dir=logging_dir,\n","    logging_steps=1000, #5000~10000\n","    eval_steps=1000,\n","    save_steps=1000,\n","    save_total_limit = 1,\n","    logging_strategy=\"steps\",\n","    save_strategy=\"steps\",\n","    eval_strategy=\"steps\",\n","    seed=42,\n","    eval_accumulation_steps = 4,\n","    logging_first_step=True\n","    # fp16=True, -> Train with FP16 generate zeros/nan values in loss\n","    # fp16_full_eval = True,\n",")\n","\n","vocab_file = tokenizer_path + \"vocab.json\"\n","merges_file = tokenizer_path + \"merges.txt\"\n","\n","tokenizer = GPT2Tokenizer(vocab_file, merges_file)\n","tokenizer.model_max_length = model.config.n_positions\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","bos_id = tokenizer.bos_token_id\n","eos_id = tokenizer.eos_token_id\n","pad_id = tokenizer.pad_token_id\n","\n","model.resize_token_embeddings(len(tokenizer))\n","\n","data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n","\n"]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1718801606262,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"oBUxkHnlf3TH"},"outputs":[],"source":["# import perplexity\n","import torch.nn.functional as F\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","\n","    if not isinstance(logits, torch.Tensor):\n","        logits = torch.tensor(logits)\n","\n","    if not isinstance(labels, torch.Tensor):\n","        labels = torch.tensor(labels)\n","\n","    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n","    perplexity = torch.exp(loss).item()  # Ensure perplexity is a scalar\n","\n","    return {\n","        \"eval_loss\": loss.item(),\n","        \"eval_perplexity\": perplexity\n","    }"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1718801606262,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"HBbyKZFIbiX3"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    data_collator=data_collator,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["trainer.args._n_gpu = 1"]},{"cell_type":"code","execution_count":65,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":23513022,"status":"ok","timestamp":1718825119280,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"JQREV1f4wtLa","outputId":"59324791-7970-42ec-9f8b-2686be6a71ab"},"outputs":[{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='81' max='16855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [   81/16855 05:42 < 20:12:39, 0.23 it/s, Epoch 0.02/5]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3238\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3237\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m   3241\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py:3264\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   3262\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3263\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 3264\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3265\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   3266\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   3267\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1302\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1302\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1317\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:1116\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1105\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1106\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m         output_attentions,\n\u001b[1;32m   1114\u001b[0m     )\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:614\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    612\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    613\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 614\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    623\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[53], line 162\u001b[0m, in \u001b[0;36mInfiniAttentionGPT2.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, use_cache, output_attentions, debug)\u001b[0m\n\u001b[1;32m    160\u001b[0m     attention_mask_segment \u001b[38;5;241m=\u001b[39m attention_mask[:, :, :, : \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msegment_size]\n\u001b[1;32m    161\u001b[0m \u001b[38;5;66;03m# print(f\"{attention_mask_segment.shape=}\")\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask_segment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m a_dot \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m final_attn\u001b[38;5;241m.\u001b[39mappend(attn_outputs[\u001b[38;5;241m1\u001b[39m])\n","File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py:195\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(heads)\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruned_heads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpruned_heads\u001b[38;5;241m.\u001b[39munion(heads)\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_attn\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, key, value, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, head_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    196\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(query, key\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_attn_weights:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1168,"status":"ok","timestamp":1718825120436,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"CHkztkHvrZf1"},"outputs":[],"source":["# save trainer\n","# trainer.save_model(model_save_dir + 'trainer/')\n","\n","torch.save(model.state_dict(), \"../models/infini.pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4595,"status":"ok","timestamp":1718825125028,"user":{"displayName":"Matheus Rodrigues","userId":"10209371041900019376"},"user_tz":180},"id":"BmZYvXsWmeiw","outputId":"076f5852-f48f-4a22-a3ec-09beed2035ff"},"outputs":[{"data":{"text/plain":["('/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/tokenizer_config.json',\n"," '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/special_tokens_map.json',\n"," '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/vocab.json',\n"," '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/merges.txt',\n"," '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/added_tokens.json')"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# trainer.model.save_pretrained(model_save_dir)\n","tokenizer.save_pretrained(model_save_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","with open(\"../models/metric.json\", \"w\") as f: \n","    json.dump(\n","        trainer.state.log_history,\n","        f,\n","        indent=2\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["### Load saved model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Parameter containing:\n","tensor([0.0005], requires_grad=True)\n","Parameter containing:\n","tensor([-0.0007], requires_grad=True)\n","Parameter containing:\n","tensor([-0.0003], requires_grad=True)\n","Parameter containing:\n","tensor([-0.0005], requires_grad=True)\n","Parameter containing:\n","tensor([-0.0003], requires_grad=True)\n","Parameter containing:\n","tensor([0.0002], requires_grad=True)\n","Parameter containing:\n","tensor([0.0002], requires_grad=True)\n","Parameter containing:\n","tensor([0.0004], requires_grad=True)\n","Parameter containing:\n","tensor([0.0004], requires_grad=True)\n","Parameter containing:\n","tensor([0.0004], requires_grad=True)\n","Parameter containing:\n","tensor([0.0003], requires_grad=True)\n","Parameter containing:\n","tensor([0.0005], requires_grad=True)\n"]}],"source":["model = GPT2LMHeadModel(config)\n","\n","for i, layer in enumerate(model.transformer.h):\n","    model.transformer.h[i].attn = InfiniAttentionGPT2(\n","        config, layer_idx=i\n","    )\n","\n","model.load_state_dict(torch.load(\"../models/infini.pt\"))"]},{"cell_type":"markdown","metadata":{"id":"GOPT212nuVQl"},"source":["### Inference trainer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BcLOyOs5vD-M"},"outputs":[],"source":["\n","def generate_infini(model_infini, tokenizer, text=\"Este Ã© um carro\", tokens_gen=10):\n","\n","    model_infini.eval()\n","\n","    previous_token_id = None\n","\n","    for _ in range(tokens_gen):\n","\n","        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n","        input_ids = inputs.input_ids.to(device)\n","        attention_mask = inputs.attention_mask.to(device)\n","\n","        outputs = model_infini(input_ids, attention_mask=attention_mask)\n","\n","        # get next token\n","\n","        next_token_logits = outputs[0][:, -1, :]\n","        next_token_id = torch.argmax(next_token_logits, dim=-1)\n","\n","        if previous_token_id == next_token_id:\n","            break\n","        else:\n","            previous_token_id = next_token_id\n","\n","        # add to input_ids\n","\n","        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n","        text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n","\n","    return text\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FjFXlefCv1dN"},"outputs":[],"source":["print(generate_infini(model, tokenizer, text=\"Meu nome Ã© Pe\", tokens_gen=10))\n","print(generate_infini(model, tokenizer, text=\"Um carro pass\", tokens_gen=10))\n","print(generate_infini(model, tokenizer, text=\"MÃºsica\", tokens_gen=10))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
