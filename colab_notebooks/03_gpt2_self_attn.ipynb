{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8DQeHelDd3v"
      },
      "source": [
        "# Training GPT-2 Model with InfiniAttention Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "joDmKmZqD7qG"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install accelerate -U\n",
        "# !pip install transformers -U\n",
        "# !pip install zarr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqToYBGlRDgP",
        "outputId": "d115bfe5-0869-43c7-b30a-4b49d428d941"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF_J-n4_Rayu"
      },
      "outputs": [],
      "source": [
        "zarr_file_path = '/content/drive/MyDrive/final_project/dataset_copy.zarr'\n",
        "tokenizer_path = '/content/drive/MyDrive/final_project/tokenizer/'\n",
        "# config_path = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/configs/config.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8csEh04qRtgQ"
      },
      "outputs": [],
      "source": [
        "# import zarr\n",
        "\n",
        "# zarr_store = zarr.load(zarr_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-wprKVYDd3y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
        "from typing import Optional, Tuple, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oojHktmSDd3z"
      },
      "source": [
        "### Standard GPT2LMHeadModel structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WKZ8taQDd30"
      },
      "outputs": [],
      "source": [
        "config = GPT2Config()\n",
        "# model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHbv9CxSDd32"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, Conv1D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ig48AZuDd33"
      },
      "source": [
        "### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ3Jo6efDd33"
      },
      "outputs": [],
      "source": [
        "model_type = \"gpt2\" #or \"gpt2-infini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qnjTscDd34"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E7fzdYItj6U"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ula9E0Slk0CA"
      },
      "outputs": [],
      "source": [
        "# gpt-2 original\n",
        "model = GPT2LMHeadModel(config).to(device)\n",
        "\n",
        "# gpt-2 infini\n",
        "# model = GPT2LMHeadModel(config)\n",
        "\n",
        "# for i, layer in enumerate(model.transformer.h):\n",
        "#     model.transformer.h[i].attn = InfiniAttentionGPT2(\n",
        "#         config, layer_idx=i\n",
        "#     )\n",
        "\n",
        "# model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9NJh8KzecKr",
        "outputId": "c962d94e-200a-489e-da6c-8884009f1a69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 124,439,808 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "path_dataset = '/content/drive/MyDrive/final_project/tokenizer/datasets/split_50/'\n",
        "\n",
        "train_dataset = Dataset.load_from_disk(path_dataset + \"train_dataset\")\n",
        "test_dataset = Dataset.load_from_disk(path_dataset + \"test_dataset\")\n",
        "\n",
        "# 50% train data and 5% of test data of 50% train data.\n",
        "train_dataset = train_dataset.select(range(int(len(train_dataset))))\n",
        "test_dataset = test_dataset.select(range(4 * 16)) # 6 * 16"
      ],
      "metadata": {
        "id": "l1cpVhS4fQn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA16JUBtpqnc",
        "outputId": "222d359c-7ec8-4b9c-d123-307fe89b85b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 269652\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXcDE55aoJT0",
        "outputId": "a5b0e54f-6abd-4840-fa29-5ce6468a9802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 64\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez6BSiCCtki1"
      },
      "outputs": [],
      "source": [
        "# train with trainer\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "output_dir = '/content/drive/MyDrive/final_project/models_self_attn/output_dir'\n",
        "logging_dir = '/content/drive/MyDrive/final_project/models_self_attn/logs'\n",
        "model_save_dir = '/content/drive/MyDrive/final_project/models_self_attn/'\n",
        "\n",
        "# batch_size = 16\n",
        "# num_epochs = 1\n",
        "\n",
        "# num_steps = len(train_dataset) * num_epochs // batch_size\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=2e-5,\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    logging_dir=logging_dir,\n",
        "    logging_steps=5_000, #5000~10000\n",
        "    eval_steps=5_000,\n",
        "    save_steps=5_000,\n",
        "    save_total_limit = 1,\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",\n",
        "    seed=42,\n",
        "    eval_accumulation_steps = 4,\n",
        "    # fp16=True, -> Train with FP16 generate zeros/nan values in loss\n",
        "    # fp16_full_eval = True,\n",
        ")\n",
        "\n",
        "vocab_file = tokenizer_path + \"vocab.json\"\n",
        "merges_file = tokenizer_path + \"merges.txt\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer(vocab_file, merges_file)\n",
        "tokenizer.model_max_length = model.config.n_positions\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bos_id = tokenizer.bos_token_id\n",
        "eos_id = tokenizer.eos_token_id\n",
        "pad_id = tokenizer.pad_token_id\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "# special tokens\n",
        "\n",
        "# add an first column of bos value\n",
        "# bos_array = np.zeros((zarr_store.shape[0], 1), dtype=np.int32)\n",
        "# bos_array[:, 0] = bos_id\n",
        "\n",
        "# add an last column of eos value\n",
        "# eos_array = np.zeros((zarr_store.shape[0], 1), dtype=np.int32)\n",
        "# eos_array[:, 0] = eos_id\n",
        "\n",
        "# zarr_store = np.concatenate((bos_array, zarr_store), axis=1)\n",
        "# zarr_store = np.concatenate((zarr_store, eos_array), axis=1)\n",
        "\n",
        "# zarr_store[:, 0] = bos_id\n",
        "# zarr_store[:, -1] = eos_id\n",
        "\n",
        "# attention mask same dimension zarr_store\n",
        "# attention_mask = np.ones(zarr_store.shape)\n",
        "\n",
        "# train 95%\n",
        "# train_size = int(zarr_store.shape[0] * 0.95)\n",
        "\n",
        "# train_input_ids = zarr_store[:train_size]\n",
        "# train_attention_mask = attention_mask[:train_size]\n",
        "\n",
        "# test 5%\n",
        "# test_input_ids = zarr_store[train_size:]\n",
        "# test_attention_mask = attention_mask[train_size:]\n",
        "\n",
        "# inputs_train = {\"input_ids\": torch.from_numpy(train_input_ids), \"attention_mask\": torch.from_numpy(train_attention_mask)}\n",
        "# inputs_test = {\"input_ids\": torch.from_numpy(test_input_ids), \"attention_mask\": torch.from_numpy(test_attention_mask)}\n",
        "\n",
        "# import torch\n",
        "# inputs_train = torch.load(tokenizer_path + \"inputs_train.pt\")\n",
        "# # replace bos\n",
        "# inputs_train['input_ids'][:, 0] = bos_id\n",
        "# # replace eos\n",
        "# inputs_train['input_ids'][:, -1] = eos_id\n",
        "\n",
        "# inputs_test = torch.load(tokenizer_path + \"inputs_test.pt\")\n",
        "# # replace bos\n",
        "# inputs_test['input_ids'][:, 0] = bos_id\n",
        "# # replace eos\n",
        "# inputs_test['input_ids'][:, -1] = eos_id\n",
        "\n",
        "\n",
        "# # save inputs_train, inputs_test\n",
        "# torch.save(inputs_train, tokenizer_path + \"inputs_train.pt\")\n",
        "# torch.save(inputs_test, tokenizer_path + \"inputs_test.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ### SCRIPT TO SAVE DATASET WITHOUT USING RAM\n",
        "# import os\n",
        "# from datasets import Dataset\n",
        "# import torch\n",
        "\n",
        "# tokenizer_path = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/'\n",
        "\n",
        "# # Create dataset directories\n",
        "# train_ds_path = os.path.join(tokenizer_path, \"datasets/train\")\n",
        "# test_ds_path = os.path.join(tokenizer_path, \"datasets/test\")\n",
        "\n",
        "# os.makedirs(train_ds_path, exist_ok=True)\n",
        "# os.makedirs(test_ds_path, exist_ok=True)\n",
        "\n",
        "# # Load tensors\n",
        "# train_ids = torch.load(tokenizer_path + \"inputs_train.pt\")\n",
        "# test_ids = torch.load(tokenizer_path + \"inputs_test.pt\")\n",
        "\n",
        "# # Function to save datasets in batches\n",
        "# def save_dataset_in_batches(ids, path, batch_size=100000):\n",
        "#     total_batches = (len(ids['input_ids']) + batch_size - 1) // batch_size  # Compute number of batches\n",
        "#     for i in range(total_batches):\n",
        "#         start_idx = i * batch_size\n",
        "#         end_idx = min((i + 1) * batch_size, len(ids['input_ids']))\n",
        "#         batch = {key: value[start_idx:end_idx] for key, value in ids.items()}\n",
        "#         dataset = Dataset.from_dict(batch)\n",
        "#         dataset.save_to_disk(os.path.join(path, f\"batch_{i:03d}\"))\n",
        "\n",
        "# # Save datasets\n",
        "# save_dataset_in_batches(train_ids, train_ds_path)\n",
        "# save_dataset_in_batches(test_ids, test_ds_path)"
      ],
      "metadata": {
        "id": "cscFJcRDHXS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from datasets import Dataset, load_from_disk\n",
        "# from tqdm import tqdm\n",
        "# import datasets\n",
        "\n",
        "# # Function to iteratively concatenate batch datasets in a directory into a single dataset\n",
        "# def concatenate_batches_iteratively(directory):\n",
        "#     batch_files = sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.startswith(\"batch_\")])\n",
        "#     cumulative_dataset = None\n",
        "\n",
        "#     # Use tqdm to display a progress bar\n",
        "#     for batch_file in tqdm(batch_files, desc=\"Loading and concatenating batches\"):\n",
        "#         current_batch = load_from_disk(batch_file)\n",
        "#         # current_batch.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "#         if cumulative_dataset is None:\n",
        "#             cumulative_dataset = current_batch\n",
        "#         else:\n",
        "#             # Concatenate the current batch with the cumulative dataset\n",
        "#             cumulative_dataset = datasets.concatenate_datasets([cumulative_dataset, current_batch])\n",
        "\n",
        "#     return cumulative_dataset\n",
        "\n",
        "# # Path to the directories containing the batch files\n",
        "# # tokenizer_path = \"path/to/your/tokenizer/\"  # Set this to the correct path\n",
        "# train_ds_path = os.path.join(tokenizer_path, \"datasets/train\")\n",
        "# test_ds_path = os.path.join(tokenizer_path, \"datasets/test\")\n",
        "\n",
        "# # Concatenate batches into a single dataset for both train and test, using the iterative function\n",
        "# train_dataset = concatenate_batches_iteratively(train_ds_path)\n",
        "# test_dataset = concatenate_batches_iteratively(test_ds_path)\n",
        "\n",
        "# # Example of usage\n",
        "# print(\"Combined train dataset:\", train_dataset)\n",
        "# print(\"Combined test dataset:\", test_dataset)"
      ],
      "metadata": {
        "id": "5nlFmTi1J0Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save datasets\n",
        "# train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# train_dataset.save_to_disk(tokenizer_path + \"train_dataset\")\n",
        "\n",
        "# test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# test_dataset.save_to_disk(tokenizer_path + \"test_dataset\")"
      ],
      "metadata": {
        "id": "JB5c7_UJMqFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7WnSCQNZTJ2"
      },
      "outputs": [],
      "source": [
        "# # from datasets import Dataset\n",
        "# # import torch\n",
        "# # # load dataset\n",
        "\n",
        "# path_dataset = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_10/'\n",
        "\n",
        "# train_dataset = Dataset.load_from_disk(path_dataset + \"train_dataset\")\n",
        "# test_dataset = Dataset.load_from_disk(path_dataset + \"test_dataset\")\n",
        "\n",
        "# test_dataset = test_dataset.select(range(int(len(test_dataset) * 0.25)))\n",
        "\n",
        "\n",
        "# # train_dataset = Dataset.from_dict(torch.load(tokenizer_path + \"inputs_train.pt\"))\n",
        "# # train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# # train_dataset.save_to_disk(tokenizer_path + \"train_dataset\")\n",
        "\n",
        "# # train_dataset = Dataset.from_dict(torch.load(tokenizer_path + \"inputs_test.pt\"))\n",
        "# # train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# # train_dataset.save_to_disk(tokenizer_path + \"test_dataset\")\n",
        "\n",
        "# # # replace column bos\n",
        "# # train_dataset['input_ids'][:, 0] = bos_id\n",
        "# # # replace column eos\n",
        "# # train_dataset['input_ids'][:, -1] = eos_id\n",
        "\n",
        "# # test_dataset = Dataset.from_dict(torch.load(tokenizer_path + \"inputs_test.pt\"))\n",
        "# # test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# # # replace column bos\n",
        "# # test_dataset['input_ids'][:, 0] = bos_id\n",
        "# # # replace column eos\n",
        "# # test_dataset['input_ids'][:, -1] = eos_id\n",
        "\n",
        "# # # Save datasets\n",
        "# # # train_dataset.save_to_disk(tokenizer_path + \"train_dataset\")\n",
        "# # # test_dataset.save_to_disk(tokenizer_path + \"test_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # import datasets\n",
        "\n",
        "# dataset = datasets.concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "# # reduce 50%\n",
        "# dataset = dataset.select(range(int(len(dataset) * 0.5)))\n",
        "\n",
        "# # 98% train, 2% test\n",
        "# train_dataset = dataset.train_test_split(test_size=0.02)\n",
        "\n",
        "# test_dataset = train_dataset.pop(\"test\")\n",
        "# train_dataset = train_dataset[\"train\"]"
      ],
      "metadata": {
        "id": "HRTOZGe9XRRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_dataset.save_to_disk('/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_50/train_dataset')\n",
        "# test_dataset.save_to_disk('/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_50/test_dataset')"
      ],
      "metadata": {
        "id": "bz0_JrqZXXjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBUxkHnlf3TH"
      },
      "outputs": [],
      "source": [
        "# import perplexity\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    if not isinstance(logits, torch.Tensor):\n",
        "        logits = torch.tensor(logits)\n",
        "\n",
        "    if not isinstance(labels, torch.Tensor):\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "    perplexity = torch.exp(loss).item()  # Ensure perplexity is a scalar\n",
        "\n",
        "    return {\n",
        "        \"eval_loss\": loss.item(),\n",
        "        \"eval_perplexity\": perplexity\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # # DEBUG\n",
        "# # # get 10% of train_dataset and 1% of test_dataset\n",
        "# train_dataset = train_dataset.select(range(int(len(train_dataset) * 0.1)))\n",
        "# test_dataset = test_dataset.select(range(int(len(test_dataset) * 0.5)))"
      ],
      "metadata": {
        "id": "wokU56TkRk4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBbyKZFIbiX3"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQREV1f4wtLa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "bb71c97f-f1ef-4123-f640-9aefcf017f64"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='33707' max='33707' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [33707/33707 5:49:15, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Perplexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>6.596000</td>\n",
              "      <td>5.833317</td>\n",
              "      <td>10995.932617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>5.657000</td>\n",
              "      <td>5.251013</td>\n",
              "      <td>12436.422852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>5.258100</td>\n",
              "      <td>4.931976</td>\n",
              "      <td>14730.146484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>5.013500</td>\n",
              "      <td>4.709438</td>\n",
              "      <td>16989.417969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>4.840300</td>\n",
              "      <td>4.558650</td>\n",
              "      <td>19057.517578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>4.727000</td>\n",
              "      <td>4.480994</td>\n",
              "      <td>20543.910156</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=33707, training_loss=5.274952045383748, metrics={'train_runtime': 20956.8962, 'train_samples_per_second': 12.867, 'train_steps_per_second': 1.608, 'total_flos': 1.40915838025728e+17, 'train_loss': 5.274952045383748, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save trainer\n",
        "trainer.save_model(model_save_dir + 'trainer/')"
      ],
      "metadata": {
        "id": "CHkztkHvrZf1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "e66d67b7-a9b1-49d6-d587-b3bba97806d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0952f50e7500>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# save trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'trainer/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.model.save_pretrained(model_save_dir)\n",
        "tokenizer.save_pretrained(model_save_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmZYvXsWmeiw",
        "outputId": "d8ae4365-4ee8-41a0-cc8d-69d247ca191d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/final_project/models_self_attn/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/final_project/models_self_attn/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/final_project/models_self_attn/vocab.json',\n",
              " '/content/drive/MyDrive/final_project/models_self_attn/merges.txt',\n",
              " '/content/drive/MyDrive/final_project/models_self_attn/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "JANaj1W4mWS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOPT212nuVQl"
      },
      "source": [
        "### Inference trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcLOyOs5vD-M"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_infini(model_infini, tokenizer, text=\"Este é um carro\", tokens_gen=10):\n",
        "\n",
        "    model_infini.eval()\n",
        "\n",
        "    previous_token_id = None\n",
        "\n",
        "    for _ in range(tokens_gen):\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "        attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "        outputs = model_infini(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # get next token\n",
        "\n",
        "        next_token_logits = outputs[0][:, -1, :]\n",
        "        next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "        if previous_token_id == next_token_id:\n",
        "            break\n",
        "        else:\n",
        "            previous_token_id = next_token_id\n",
        "\n",
        "        # add to input_ids\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "        text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjFXlefCv1dN"
      },
      "outputs": [],
      "source": [
        "print(generate_infini(model, tokenizer, text=\"Meu nome é Pe\", tokens_gen=10))\n",
        "print(generate_infini(model, tokenizer, text=\"Um carro pass\", tokens_gen=10))\n",
        "print(generate_infini(model, tokenizer, text=\"Música\", tokens_gen=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CYhoZKhzcQO"
      },
      "outputs": [],
      "source": [
        "# DEBUG GENERATE\n",
        "# inputs = tokenizer(\"LAR\",return_tensors=\"pt\", truncation=True)\n",
        "# input_ids = inputs.input_ids.to(device)\n",
        "# attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "# model.generate(input_ids, max_new_tokens=10, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDcNTAIGnIEN"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_config(config)\n",
        "y = model(input_ids, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc_yCsxxol-3"
      },
      "outputs": [],
      "source": [
        "y.past_key_values[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY8y-ThR99xW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}