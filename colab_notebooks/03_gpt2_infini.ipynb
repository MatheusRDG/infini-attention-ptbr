{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8DQeHelDd3v"
      },
      "source": [
        "# Training GPT-2 Model with InfiniAttention Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "joDmKmZqD7qG"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets\n",
        "# !pip install accelerate -U\n",
        "# !pip install transformers -U\n",
        "# !pip install zarr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqToYBGlRDgP",
        "outputId": "d8bd675c-2919-4929-a684-84afc0af429d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF_J-n4_Rayu"
      },
      "outputs": [],
      "source": [
        "zarr_file_path = (\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/dataset_copy.zarr\"\n",
        ")\n",
        "tokenizer_path = (\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/\"\n",
        ")\n",
        "# config_path = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/configs/config.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8csEh04qRtgQ"
      },
      "outputs": [],
      "source": [
        "# import zarr\n",
        "\n",
        "# zarr_store = zarr.load(zarr_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "x-wprKVYDd3y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
        "from typing import Optional, Tuple, Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oojHktmSDd3z"
      },
      "source": [
        "### Standard GPT2LMHeadModel structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WKZ8taQDd30"
      },
      "outputs": [],
      "source": [
        "config = GPT2Config()\n",
        "# model = GPT2LMHeadModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHbv9CxSDd32"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, Conv1D\n",
        "\n",
        "\n",
        "class InfiniAttentionGPT2(GPT2Attention):\n",
        "    def __init__(\n",
        "        self,\n",
        "        config,\n",
        "        is_cross_attention=False,\n",
        "        layer_idx=None,\n",
        "        n_segments=16,\n",
        "        is_causal: Optional[bool] = True,\n",
        "        update: Optional[str] = \"linear\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the InfiniAttentionGPT2 module.\n",
        "\n",
        "        Args:\n",
        "            config: GPT2Config object containing configuration parameters.\n",
        "            is_cross_attention (bool): Flag to indicate if this is cross attention.\n",
        "            layer_idx (int, optional): Index of the layer.\n",
        "            n_segments (int): Number of segments for memory processing.\n",
        "            is_causal (bool, optional): Flag to indicate if attention is causal.\n",
        "            update (str, optional): Update strategy for memory, either 'linear' or another strategy.\n",
        "        \"\"\"\n",
        "        super().__init__(config, is_cross_attention, layer_idx)\n",
        "\n",
        "        # Initializing memory state for compressive memory\n",
        "        self.d_head = config.hidden_size // config.num_attention_heads\n",
        "        self.n_head = config.num_attention_heads\n",
        "\n",
        "        # Initialize the beta parameter for combining A_mem and A_dot\n",
        "        self.beta = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "\n",
        "        # Sequence length\n",
        "        self.seq_len = config.n_positions\n",
        "\n",
        "        self.is_causal = is_causal\n",
        "        self.register_buffer(\n",
        "            \"causal\",\n",
        "            torch.tril(\n",
        "                torch.ones(self.seq_len // n_segments, self.seq_len // n_segments)\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Segment size\n",
        "        self.n_segments = n_segments\n",
        "        self.segment_size = self.seq_len // n_segments\n",
        "\n",
        "        # Update strategy\n",
        "        self.update = update\n",
        "\n",
        "    def _retrieve_from_memory(self, query_states):\n",
        "        # Retrieve context from compressive memory using linear attention (Eq. 3)\n",
        "        if self.memory is None:\n",
        "            return torch.zeros_like(query_states)\n",
        "        query_states = F.elu(query_states) + 1  # ELU activation\n",
        "        memory_output = torch.matmul(query_states, self.memory) / self.norm_term\n",
        "        return memory_output\n",
        "\n",
        "    def _update_memory(self, key_states, value_states):\n",
        "        # Update compressive memory with new key-value states (Eq. 4)\n",
        "        key_states = F.elu(key_states) + 1  # ELU activation\n",
        "        if self.memory is not None:\n",
        "            self.memory = self.memory + torch.matmul(\n",
        "                key_states.transpose(-2, -1), value_states\n",
        "            )\n",
        "        else:\n",
        "            self.memory = torch.matmul(key_states.transpose(-2, -1), value_states)\n",
        "        if self.norm_term is not None:\n",
        "            self.norm_term = self.norm_term + torch.unsqueeze(\n",
        "                key_states.sum(dim=-2), -2\n",
        "            )\n",
        "        else:\n",
        "            self.norm_term = torch.unsqueeze(key_states.sum(dim=-2), -2)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.FloatTensor,\n",
        "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.FloatTensor] = None,\n",
        "        head_mask: Optional[torch.FloatTensor] = None,\n",
        "        use_cache: Optional[bool] = False,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        debug: Optional[bool] = False,\n",
        "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
        "\n",
        "        self.norm_term = None\n",
        "        self.memory = None\n",
        "\n",
        "        batch_size, _, _ = hidden_states.size()\n",
        "\n",
        "        device = hidden_states.device\n",
        "\n",
        "        qkv = self.c_attn(hidden_states)\n",
        "        query, key, value = qkv.split(self.split_size, dim=2)\n",
        "\n",
        "        # segments = torch.tensor_split(\n",
        "        #     hidden_states,\n",
        "        #     list(range(self.segment_size, hidden_states.size(1), self.segment_size)),\n",
        "        #     dim=1,\n",
        "        # )\n",
        "\n",
        "        segments_q = torch.tensor_split(\n",
        "            query,\n",
        "            list(range(self.segment_size, query.size(1), self.segment_size)),\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        segments_k = torch.tensor_split(\n",
        "            key,\n",
        "            list(range(self.segment_size, key.size(1), self.segment_size)),\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        segments_v = torch.tensor_split(\n",
        "            value,\n",
        "            list(range(self.segment_size, value.size(1), self.segment_size)),\n",
        "            dim=1,\n",
        "        )\n",
        "\n",
        "        final_outputs = []\n",
        "        final_k = []\n",
        "        final_attn = []\n",
        "\n",
        "        # print(f\"{hidden_states.shape=}\")\n",
        "        # print(f\"{self.c_attn(hidden_states).shape=}\")\n",
        "        # print(f\"{segments_q[0].shape=}\")\n",
        "\n",
        "        for i, segment in enumerate(segments_q):\n",
        "            # print(f\"{segment.shape=}\")\n",
        "            # qkv_segment = self.c_attn(segment)\n",
        "            # query, key, value = qkv_segment.split(self.split_size, dim=2)\n",
        "\n",
        "            query = segments_q[i]\n",
        "            key = segments_k[i]\n",
        "            value = segments_v[i]\n",
        "\n",
        "            query = self._split_heads(\n",
        "                query, num_heads=self.n_head, attn_head_size=self.d_head\n",
        "            )\n",
        "            key = self._split_heads(\n",
        "                key, num_heads=self.n_head, attn_head_size=self.d_head\n",
        "            )\n",
        "            final_k.append(key)\n",
        "            value = self._split_heads(\n",
        "                value, num_heads=self.n_head, attn_head_size=self.d_head\n",
        "            )\n",
        "\n",
        "            bsz, q_len, _ = segment.size()\n",
        "\n",
        "            # print(f\"{query.shape=}\")\n",
        "            memory_output = self._retrieve_from_memory(query)\n",
        "            self._update_memory(key, value)\n",
        "\n",
        "            # print(f\"{attention_mask.shape=}\")\n",
        "            if attention_mask is not None:\n",
        "                attention_mask_segment = attention_mask[:, :, :, : self.segment_size]\n",
        "            # print(f\"{attention_mask_segment.shape=}\")\n",
        "            attn_outputs = self._attn(\n",
        "                query, key, value, attention_mask_segment, head_mask\n",
        "            )\n",
        "            a_dot = attn_outputs[0]\n",
        "            final_attn.append(attn_outputs[1])\n",
        "\n",
        "            # attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
        "            #     query_states,\n",
        "            #     key_states,\n",
        "            #     value_states,\n",
        "            #     attn_mask=causal_mask,\n",
        "            #     dropout_p=self.attention_dropout if self.training else 0.0,\n",
        "            # )\n",
        "\n",
        "            combined_output = (\n",
        "                F.sigmoid(self.beta) * memory_output\n",
        "                + (1 - F.sigmoid(self.beta)) * a_dot\n",
        "            )\n",
        "\n",
        "            combined_output = self._merge_heads(\n",
        "                combined_output, self.n_head, self.d_head\n",
        "            )\n",
        "            combined_output = self.c_proj(combined_output)\n",
        "            combined_output = self.resid_dropout(combined_output)\n",
        "\n",
        "            final_outputs.append(combined_output)\n",
        "\n",
        "        final_outputs = torch.cat(final_outputs, dim=1)\n",
        "        final_k = torch.cat(final_k, dim=1)\n",
        "        final_attn = torch.cat(final_attn, dim=1)\n",
        "        outputs = (final_outputs, final_k)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs = outputs + (final_attn,)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "# # Supondo que o `config`, `hidden_states`, e `attention_mask` já tenham sido definidos.\n",
        "# config = GPT2Config()\n",
        "# config.n_positions = 1024  # Definindo um exemplo de tamanho de sequência\n",
        "# hidden_states = torch.randn(2, 1024, config.hidden_size)  # Exemplo de estados ocultos\n",
        "# attention_mask = torch.ones(2, 1024)  # Exemplo de máscara de atenção\n",
        "# attention_mask[:, :512] = 0  # Máscara de atenção para metade da sequência\n",
        "\n",
        "\n",
        "# infini_att_gpt2 = InfiniAttentionGPT2(config=config, is_causal=False, n_segments=16)\n",
        "\n",
        "# # Forward\n",
        "# # outputs = infini_att_gpt2(\n",
        "# #     hidden_states=hidden_states, attention_mask=attention_mask, debug=True\n",
        "# # )\n",
        "\n",
        "# # Output\n",
        "# print(\"Output InfiniAttention GPT-2 att:\")\n",
        "# # print(f\"{outputs[0].shape=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ig48AZuDd33"
      },
      "source": [
        "### Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ3Jo6efDd33"
      },
      "outputs": [],
      "source": [
        "model_type = \"gpt2-infini\"  # \"gpt2\" or \"gpt2-infini\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97qnjTscDd34"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2E7fzdYItj6U"
      },
      "source": [
        "### Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ula9E0Slk0CA"
      },
      "outputs": [],
      "source": [
        "# gpt-2 original\n",
        "# model = GPT2LMHeadModel(config).to(device)\n",
        "\n",
        "# gpt-2 infini\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "for i, layer in enumerate(model.transformer.h):\n",
        "    model.transformer.h[i].attn = InfiniAttentionGPT2(config, layer_idx=i)\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9NJh8KzecKr",
        "outputId": "0b4944d1-9f1f-4431-e11d-a5962b0d65a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 124,439,820 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1cpVhS4fQn7"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import torch\n",
        "\n",
        "path_dataset = \"/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_10/\"\n",
        "\n",
        "train_dataset = Dataset.load_from_disk(path_dataset + \"train_dataset\")\n",
        "test_dataset = Dataset.load_from_disk(path_dataset + \"test_dataset\")\n",
        "\n",
        "# 50% train data and 5% of test data of 50% train data.\n",
        "train_dataset = train_dataset.select(range(int(len(train_dataset))))\n",
        "test_dataset = test_dataset.select(range(4 * 16))  # 6 * 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA16JUBtpqnc",
        "outputId": "ee4985eb-7892-48f5-e500-b7c92c340cc2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 53930\n",
              "})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXcDE55aoJT0",
        "outputId": "e815ee60-6da2-4c24-f094-2f0b4cf9b491"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask'],\n",
              "    num_rows: 64\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez6BSiCCtki1"
      },
      "outputs": [],
      "source": [
        "# train with trainer\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers import DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "\n",
        "output_dir = (\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/output_dir\"\n",
        ")\n",
        "logging_dir = (\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/logs\"\n",
        ")\n",
        "model_save_dir = (\n",
        "    \"/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/\"\n",
        ")\n",
        "\n",
        "# batch_size = 16\n",
        "# num_epochs = 1\n",
        "\n",
        "# num_steps = len(train_dataset) * num_epochs // batch_size\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    learning_rate=2e-5,\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=logging_dir,\n",
        "    logging_steps=1000,  # 5000~10000\n",
        "    eval_steps=500,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=1,\n",
        "    logging_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    eval_strategy=\"steps\",\n",
        "    seed=42,\n",
        "    eval_accumulation_steps=4,\n",
        "    # fp16=True, -> Train with FP16 generate zeros/nan values in loss\n",
        "    # fp16_full_eval = True,\n",
        ")\n",
        "\n",
        "vocab_file = tokenizer_path + \"vocab.json\"\n",
        "merges_file = tokenizer_path + \"merges.txt\"\n",
        "\n",
        "tokenizer = GPT2Tokenizer(vocab_file, merges_file)\n",
        "tokenizer.model_max_length = model.config.n_positions\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bos_id = tokenizer.bos_token_id\n",
        "eos_id = tokenizer.eos_token_id\n",
        "pad_id = tokenizer.pad_token_id\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "\n",
        "# special tokens\n",
        "\n",
        "# add an first column of bos value\n",
        "# bos_array = np.zeros((zarr_store.shape[0], 1), dtype=np.int32)\n",
        "# bos_array[:, 0] = bos_id\n",
        "\n",
        "# add an last column of eos value\n",
        "# eos_array = np.zeros((zarr_store.shape[0], 1), dtype=np.int32)\n",
        "# eos_array[:, 0] = eos_id\n",
        "\n",
        "# zarr_store = np.concatenate((bos_array, zarr_store), axis=1)\n",
        "# zarr_store = np.concatenate((zarr_store, eos_array), axis=1)\n",
        "\n",
        "# zarr_store[:, 0] = bos_id\n",
        "# zarr_store[:, -1] = eos_id\n",
        "\n",
        "# attention mask same dimension zarr_store\n",
        "# attention_mask = np.ones(zarr_store.shape)\n",
        "\n",
        "# train 95%\n",
        "# train_size = int(zarr_store.shape[0] * 0.95)\n",
        "\n",
        "# train_input_ids = zarr_store[:train_size]\n",
        "# train_attention_mask = attention_mask[:train_size]\n",
        "\n",
        "# test 5%\n",
        "# test_input_ids = zarr_store[train_size:]\n",
        "# test_attention_mask = attention_mask[train_size:]\n",
        "\n",
        "# inputs_train = {\"input_ids\": torch.from_numpy(train_input_ids), \"attention_mask\": torch.from_numpy(train_attention_mask)}\n",
        "# inputs_test = {\"input_ids\": torch.from_numpy(test_input_ids), \"attention_mask\": torch.from_numpy(test_attention_mask)}\n",
        "\n",
        "# import torch\n",
        "# inputs_train = torch.load(tokenizer_path + \"inputs_train.pt\")\n",
        "# # replace bos\n",
        "# inputs_train['input_ids'][:, 0] = bos_id\n",
        "# # replace eos\n",
        "# inputs_train['input_ids'][:, -1] = eos_id\n",
        "\n",
        "# inputs_test = torch.load(tokenizer_path + \"inputs_test.pt\")\n",
        "# # replace bos\n",
        "# inputs_test['input_ids'][:, 0] = bos_id\n",
        "# # replace eos\n",
        "# inputs_test['input_ids'][:, -1] = eos_id\n",
        "\n",
        "\n",
        "# # save inputs_train, inputs_test\n",
        "# torch.save(inputs_train, tokenizer_path + \"inputs_train.pt\")\n",
        "# torch.save(inputs_test, tokenizer_path + \"inputs_test.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# test data_collator\n",
        "\n",
        "data_collator(\n",
        "    train_dataset[:2],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cscFJcRDHXS-"
      },
      "outputs": [],
      "source": [
        "# ### SCRIPT TO SAVE DATASET WITHOUT USING RAM\n",
        "# import os\n",
        "# from datasets import Dataset\n",
        "# import torch\n",
        "\n",
        "# tokenizer_path = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/'\n",
        "\n",
        "# # Create dataset directories\n",
        "# train_ds_path = os.path.join(tokenizer_path, \"datasets/train\")\n",
        "# test_ds_path = os.path.join(tokenizer_path, \"datasets/test\")\n",
        "\n",
        "# os.makedirs(train_ds_path, exist_ok=True)\n",
        "# os.makedirs(test_ds_path, exist_ok=True)\n",
        "\n",
        "# # Load tensors\n",
        "# train_ids = torch.load(tokenizer_path + \"inputs_train.pt\")\n",
        "# test_ids = torch.load(tokenizer_path + \"inputs_test.pt\")\n",
        "\n",
        "# # Function to save datasets in batches\n",
        "# def save_dataset_in_batches(ids, path, batch_size=100000):\n",
        "#     total_batches = (len(ids['input_ids']) + batch_size - 1) // batch_size  # Compute number of batches\n",
        "#     for i in range(total_batches):\n",
        "#         start_idx = i * batch_size\n",
        "#         end_idx = min((i + 1) * batch_size, len(ids['input_ids']))\n",
        "#         batch = {key: value[start_idx:end_idx] for key, value in ids.items()}\n",
        "#         dataset = Dataset.from_dict(batch)\n",
        "#         dataset.save_to_disk(os.path.join(path, f\"batch_{i:03d}\"))\n",
        "\n",
        "# # Save datasets\n",
        "# save_dataset_in_batches(train_ids, train_ds_path)\n",
        "# save_dataset_in_batches(test_ids, test_ds_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nlFmTi1J0Fu"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# from datasets import Dataset, load_from_disk\n",
        "# from tqdm import tqdm\n",
        "# import datasets\n",
        "\n",
        "# # Function to iteratively concatenate batch datasets in a directory into a single dataset\n",
        "# def concatenate_batches_iteratively(directory):\n",
        "#     batch_files = sorted([os.path.join(directory, f) for f in os.listdir(directory) if f.startswith(\"batch_\")])\n",
        "#     cumulative_dataset = None\n",
        "\n",
        "#     # Use tqdm to display a progress bar\n",
        "#     for batch_file in tqdm(batch_files, desc=\"Loading and concatenating batches\"):\n",
        "#         current_batch = load_from_disk(batch_file)\n",
        "#         # current_batch.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "#         if cumulative_dataset is None:\n",
        "#             cumulative_dataset = current_batch\n",
        "#         else:\n",
        "#             # Concatenate the current batch with the cumulative dataset\n",
        "#             cumulative_dataset = datasets.concatenate_datasets([cumulative_dataset, current_batch])\n",
        "\n",
        "#     return cumulative_dataset\n",
        "\n",
        "# # Path to the directories containing the batch files\n",
        "# # tokenizer_path = \"path/to/your/tokenizer/\"  # Set this to the correct path\n",
        "# train_ds_path = os.path.join(tokenizer_path, \"datasets/train\")\n",
        "# test_ds_path = os.path.join(tokenizer_path, \"datasets/test\")\n",
        "\n",
        "# # Concatenate batches into a single dataset for both train and test, using the iterative function\n",
        "# train_dataset = concatenate_batches_iteratively(train_ds_path)\n",
        "# test_dataset = concatenate_batches_iteratively(test_ds_path)\n",
        "\n",
        "# # Example of usage\n",
        "# print(\"Combined train dataset:\", train_dataset)\n",
        "# print(\"Combined test dataset:\", test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB5c7_UJMqFv"
      },
      "outputs": [],
      "source": [
        "# save datasets\n",
        "# train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# train_dataset.save_to_disk(tokenizer_path + \"train_dataset\")\n",
        "\n",
        "# test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# test_dataset.save_to_disk(tokenizer_path + \"test_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7WnSCQNZTJ2"
      },
      "outputs": [],
      "source": [
        "# # from datasets import Dataset\n",
        "# # import torch\n",
        "# # # load dataset\n",
        "\n",
        "# path_dataset = '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_10/'\n",
        "\n",
        "# train_dataset = Dataset.load_from_disk(path_dataset + \"train_dataset\")\n",
        "# test_dataset = Dataset.load_from_disk(path_dataset + \"test_dataset\")\n",
        "\n",
        "# test_dataset = test_dataset.select(range(int(len(test_dataset) * 0.25)))\n",
        "\n",
        "\n",
        "# # train_dataset = Dataset.from_dict(torch.load(tokenizer_path + \"inputs_train.pt\"))\n",
        "# # train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# # train_dataset.save_to_disk(tokenizer_path + \"train_dataset\")\n",
        "\n",
        "# # train_dataset = Dataset.from_dict(torch.load(tokenizer_path + \"inputs_test.pt\"))\n",
        "# # train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "# # train_dataset.save_to_disk(tokenizer_path + \"test_dataset\")\n",
        "\n",
        "# # # replace column bos\n",
        "# # train_dataset['input_ids'][:, 0] = bos_id\n",
        "# # # replace column eos\n",
        "# # train_dataset['input_ids'][:, -1] = eos_id\n",
        "\n",
        "# # test_dataset = Dataset.from_dict(torch.load(tokenizer_path + \"inputs_test.pt\"))\n",
        "# # test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
        "\n",
        "# # # replace column bos\n",
        "# # test_dataset['input_ids'][:, 0] = bos_id\n",
        "# # # replace column eos\n",
        "# # test_dataset['input_ids'][:, -1] = eos_id\n",
        "\n",
        "# # # Save datasets\n",
        "# # # train_dataset.save_to_disk(tokenizer_path + \"train_dataset\")\n",
        "# # # test_dataset.save_to_disk(tokenizer_path + \"test_dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRTOZGe9XRRh"
      },
      "outputs": [],
      "source": [
        "# # import datasets\n",
        "\n",
        "# dataset = datasets.concatenate_datasets([train_dataset, test_dataset])\n",
        "\n",
        "# # reduce 50%\n",
        "# dataset = dataset.select(range(int(len(dataset) * 0.5)))\n",
        "\n",
        "# # 98% train, 2% test\n",
        "# train_dataset = dataset.train_test_split(test_size=0.02)\n",
        "\n",
        "# test_dataset = train_dataset.pop(\"test\")\n",
        "# train_dataset = train_dataset[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz0_JrqZXXjg"
      },
      "outputs": [],
      "source": [
        "# train_dataset.save_to_disk('/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_50/train_dataset')\n",
        "# test_dataset.save_to_disk('/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/tokenizer/datasets/split_50/test_dataset')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBUxkHnlf3TH"
      },
      "outputs": [],
      "source": [
        "# import perplexity\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "\n",
        "    if not isinstance(logits, torch.Tensor):\n",
        "        logits = torch.tensor(logits)\n",
        "\n",
        "    if not isinstance(labels, torch.Tensor):\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    # GPT-2 LM Head Cross Entropy Loss\n",
        "    loss = F.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
        "\n",
        "    # GPT-2 LM Head Perplexity\n",
        "    perplexity = torch.exp(loss)\n",
        "    return {\"eval_loss\": loss.item(), \"eval_perplexity\": perplexity}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wokU56TkRk4f"
      },
      "outputs": [],
      "source": [
        "# # # DEBUG\n",
        "# # # get 10% of train_dataset and 1% of test_dataset\n",
        "# train_dataset = train_dataset.select(range(int(len(train_dataset) * 0.1)))\n",
        "# test_dataset = test_dataset.select(range(int(len(test_dataset) * 0.5)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBbyKZFIbiX3"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    data_collator=data_collator,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JQREV1f4wtLa",
        "outputId": "59324791-7970-42ec-9f8b-2686be6a71ab"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16855' max='16855' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16855/16855 6:31:50, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Perplexity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>No log</td>\n",
              "      <td>7.997762</td>\n",
              "      <td>2953.017822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>8.077600</td>\n",
              "      <td>7.707036</td>\n",
              "      <td>6190.458008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>8.077600</td>\n",
              "      <td>7.433079</td>\n",
              "      <td>6643.856445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>7.435400</td>\n",
              "      <td>7.227094</td>\n",
              "      <td>6549.385742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>7.435400</td>\n",
              "      <td>7.056933</td>\n",
              "      <td>8408.976562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>7.044600</td>\n",
              "      <td>6.926828</td>\n",
              "      <td>9471.450195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>7.044600</td>\n",
              "      <td>6.817666</td>\n",
              "      <td>9934.422852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>6.795400</td>\n",
              "      <td>6.716465</td>\n",
              "      <td>10777.864258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>6.795400</td>\n",
              "      <td>6.633573</td>\n",
              "      <td>10195.775391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>6.607000</td>\n",
              "      <td>6.567185</td>\n",
              "      <td>11136.291016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>6.607000</td>\n",
              "      <td>6.504904</td>\n",
              "      <td>10453.868164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>6.480100</td>\n",
              "      <td>6.455844</td>\n",
              "      <td>10911.338867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>6.480100</td>\n",
              "      <td>6.404847</td>\n",
              "      <td>10575.078125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>6.375000</td>\n",
              "      <td>6.351100</td>\n",
              "      <td>10712.108398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>6.375000</td>\n",
              "      <td>6.312978</td>\n",
              "      <td>10881.453125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>6.272100</td>\n",
              "      <td>6.271198</td>\n",
              "      <td>10531.700195</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>6.272100</td>\n",
              "      <td>6.229383</td>\n",
              "      <td>10369.776367</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>6.197700</td>\n",
              "      <td>6.193209</td>\n",
              "      <td>10417.115234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>6.197700</td>\n",
              "      <td>6.160833</td>\n",
              "      <td>10429.471680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>6.130500</td>\n",
              "      <td>6.130695</td>\n",
              "      <td>10197.009766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>6.130500</td>\n",
              "      <td>6.104207</td>\n",
              "      <td>10541.829102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>6.066600</td>\n",
              "      <td>6.075226</td>\n",
              "      <td>10127.937500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>6.066600</td>\n",
              "      <td>6.054402</td>\n",
              "      <td>10657.369141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>6.018100</td>\n",
              "      <td>6.035589</td>\n",
              "      <td>10537.838867</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>6.018100</td>\n",
              "      <td>6.016080</td>\n",
              "      <td>10491.631836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>5.984200</td>\n",
              "      <td>5.996735</td>\n",
              "      <td>10594.509766</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>5.984200</td>\n",
              "      <td>5.980206</td>\n",
              "      <td>10643.382812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>5.948100</td>\n",
              "      <td>5.969008</td>\n",
              "      <td>10668.412109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>5.948100</td>\n",
              "      <td>5.955691</td>\n",
              "      <td>10518.841797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>5.920300</td>\n",
              "      <td>5.947272</td>\n",
              "      <td>10510.569336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>5.920300</td>\n",
              "      <td>5.939826</td>\n",
              "      <td>10719.742188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>5.912300</td>\n",
              "      <td>5.935024</td>\n",
              "      <td>10663.295898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>5.912300</td>\n",
              "      <td>5.932550</td>\n",
              "      <td>10747.379883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=16855, training_loss=6.425810335397508, metrics={'train_runtime': 23512.7091, 'train_samples_per_second': 11.468, 'train_steps_per_second': 0.717, 'total_flos': 1.409148127383552e+17, 'train_loss': 6.425810335397508, 'epoch': 5.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# get all params model\n",
        "\n",
        "for layer in model.transformer.h:\n",
        "    print(layer.attn.beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save model with torch\n",
        "torch.save(model.state_dict(), model_save_dir + \"model_gpt2_infini.pt\")\n",
        "\n",
        "# load\n",
        "torch.load(model_save_dir + \"model_gpt2_infini.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHkztkHvrZf1"
      },
      "outputs": [],
      "source": [
        "# save trainer\n",
        "trainer.save_model(model_save_dir + \"trainer/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmZYvXsWmeiw",
        "outputId": "076f5852-f48f-4a22-a3ec-09beed2035ff"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/vocab.json',\n",
              " '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/merges.txt',\n",
              " '/content/drive/MyDrive/Colab Notebooks/nlp_unicamp/final_project/models/added_tokens.json')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.model.save_pretrained(model_save_dir)\n",
        "tokenizer.save_pretrained(model_save_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JANaj1W4mWS9"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "time.sleep(5)\n",
        "\n",
        "from google.colab import runtime\n",
        "\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOPT212nuVQl"
      },
      "source": [
        "### Inference trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcLOyOs5vD-M"
      },
      "outputs": [],
      "source": [
        "def generate_infini(model_infini, tokenizer, text=\"Este é um carro\", tokens_gen=10):\n",
        "\n",
        "    model_infini.eval()\n",
        "\n",
        "    previous_token_id = None\n",
        "\n",
        "    for _ in range(tokens_gen):\n",
        "\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
        "        input_ids = inputs.input_ids.to(device)\n",
        "        attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "        outputs = model_infini(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # get next token\n",
        "\n",
        "        next_token_logits = outputs[0][:, -1, :]\n",
        "        next_token_id = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "        if previous_token_id == next_token_id:\n",
        "            break\n",
        "        else:\n",
        "            previous_token_id = next_token_id\n",
        "\n",
        "        # add to input_ids\n",
        "\n",
        "        input_ids = torch.cat([input_ids, next_token_id.unsqueeze(-1)], dim=-1)\n",
        "        text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjFXlefCv1dN"
      },
      "outputs": [],
      "source": [
        "print(generate_infini(model, tokenizer, text=\"Meu nome é Pe\", tokens_gen=10))\n",
        "print(generate_infini(model, tokenizer, text=\"Um carro pass\", tokens_gen=10))\n",
        "print(generate_infini(model, tokenizer, text=\"Música\", tokens_gen=10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CYhoZKhzcQO"
      },
      "outputs": [],
      "source": [
        "# DEBUG GENERATE\n",
        "# inputs = tokenizer(\"LAR\",return_tensors=\"pt\", truncation=True)\n",
        "# input_ids = inputs.input_ids.to(device)\n",
        "# attention_mask = inputs.attention_mask.to(device)\n",
        "\n",
        "# model.generate(input_ids, max_new_tokens=10, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDcNTAIGnIEN"
      },
      "outputs": [],
      "source": [
        "model = GPT2LMHeadModel.from_config(config)\n",
        "y = model(input_ids, attention_mask=attention_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jc_yCsxxol-3"
      },
      "outputs": [],
      "source": [
        "y.past_key_values[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UY8y-ThR99xW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
