{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Modeling\n",
        "\n",
        "Notebook to generate a datasets.Dataset file for models. \\\n",
        "\n",
        "For GPT-2 the columns will be:\n",
        "- input_ids\n",
        "- attention_mask\n",
        "\n",
        "For IntiniTransformer the columns will be:\n",
        "- input_ids\n",
        "- labels\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import GPT2Config, GPT2Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_dataset_tiktoken(dataset, tokenizer, config):\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    text = \" \".join(dataset[\"text\"])\n",
        "\n",
        "    # Encode the text\n",
        "    encoded_text = tokenizer.encode(text)\n",
        "\n",
        "    # Convert to tensor\n",
        "    encoded_text = torch.tensor(encoded_text, dtype=torch.long)\n",
        "\n",
        "    # Shift to labels and remove last token\n",
        "    encoded_labels = encoded_text[1:]\n",
        "    encoded_text = encoded_text[:-1]\n",
        "\n",
        "    # split chunks\n",
        "    input_ids = torch.split(encoded_text, config.n_positions, dim=0)[:-1]\n",
        "    labels = torch.split(encoded_labels, config.n_positions, dim=0)[:-1]\n",
        "\n",
        "    assert len(input_ids) == len(labels)\n",
        "    assert set([len(x) for x in input_ids]) == {config.n_positions}\n",
        "    assert set([len(x) for x in labels]) == {config.n_positions}\n",
        "\n",
        "    train_ds = Dataset.from_dict(\n",
        "        {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"labels\": labels,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return train_ds\n",
        "\n",
        "\n",
        "def generate_dataset_gpt2tokenizer(dataset, tokenizer, config):\n",
        "    # Tokenize the dataset\n",
        "    text = dataset[\"text\"]\n",
        "\n",
        "    # Encode the text\n",
        "    encoded_text = tokenizer(\" \".join(dataset[\"text\"]), return_tensors=\"pt\")\n",
        "    input_ids = encoded_text[\"input_ids\"]\n",
        "    attention_mask = encoded_text[\"attention_mask\"]\n",
        "\n",
        "    # split chunks\n",
        "    input_ids = torch.split(input_ids, config.n_positions, dim=1)[:-1]\n",
        "    attention_mask = torch.split(attention_mask, config.n_positions, dim=1)[:-1]\n",
        "\n",
        "    assert len(input_ids) == len(attention_mask)\n",
        "    assert set([x.shape[1] for x in input_ids]) == {config.n_positions}\n",
        "    assert set([x.shape[1] for x in attention_mask]) == {config.n_positions}\n",
        "\n",
        "    train_ds = Dataset.from_dict(\n",
        "        {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return train_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parameters\n",
        "SAMPLE = 0.0001  # Partition of the dataset to sample\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"carolina-c4ai/corpus-carolina\", taxonomy=\"wik\")[\"corpus\"]\n",
        "\n",
        "# Sample dataset\n",
        "dataset = dataset.shuffle(seed=42).select(range(int(len(dataset) * SAMPLE)))\n",
        "\n",
        "# Load the GPT2 configuration\n",
        "config = GPT2Config()\n",
        "\n",
        "# Tokenizer (select the tokenizer)\n",
        "\n",
        "# GPT-2 TOKENIZERS\n",
        "# tokenizer = GPT2Tokenizer(\n",
        "#     vocab_file=\"./tokenizer/vocab.json\", merges_file=\"./tokenizer/merges.txt\"\n",
        "# ) # Custom Tokenizer\n",
        "# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\") # Original GPT-2 Tokenizer\n",
        "\n",
        "# tokenizer.model_max_length = config.n_positions\n",
        "# tokenizer.eos_token = tokenizer.bos_token\n",
        "# tokenizer.pad_token = tokenizer.eos_token\n",
        "# tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# TIKTOKEN\n",
        "tokenizer = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "if type(tokenizer) == GPT2Tokenizer:\n",
        "    train_ds = generate_dataset_gpt2tokenizer(dataset, tokenizer, config)\n",
        "elif type(tokenizer) == tiktoken.core.Encoding:\n",
        "    train_ds = generate_dataset_tiktoken(dataset, tokenizer, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'labels'],\n",
              "    num_rows: 52\n",
              "})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_ds.save_to_disk(\"data/train\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
