{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training GPT-2 Model with InfiniAttention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "from typing import Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard GPT2LMHeadModel structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = GPT2Config()\n",
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing GPT-2 Original Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output GPT-2 att:\n",
      "outputs[0].shape=torch.Size([1, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "# Instance GPT-Attention Module\n",
    "gpt2_att = GPT2Attention(config=config)\n",
    "\n",
    "# Dummy data\n",
    "batch_size = 1\n",
    "seq_length = config.n_positions\n",
    "hidden_size = config.hidden_size\n",
    "\n",
    "hidden_states = torch.rand(batch_size, seq_length, hidden_size)\n",
    "attention_mask = torch.ones(batch_size, seq_length)\n",
    "\n",
    "# Forward\n",
    "outputs = gpt2_att(hidden_states=hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "# Output\n",
    "print(\"Output GPT-2 att:\")\n",
    "print(f\"{outputs[0].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output InfiniAttention GPT-2 att:\n",
      "outputs[0].shape=torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention, Conv1D\n",
    "from transformers import GPT2Model, GPT2Config\n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "\n",
    "class InfiniAttentionGPT2(GPT2Attention):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config,\n",
    "        is_cross_attention=False,\n",
    "        layer_idx=None,\n",
    "        n_segments=16,\n",
    "        is_causal: Optional[bool] = True,\n",
    "        update: Optional[str] = \"linear\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the InfiniAttentionGPT2 module.\n",
    "\n",
    "        Args:\n",
    "            config: GPT2Config object containing configuration parameters.\n",
    "            is_cross_attention (bool): Flag to indicate if this is cross attention.\n",
    "            layer_idx (int, optional): Index of the layer.\n",
    "            n_segments (int): Number of segments for memory processing.\n",
    "            is_causal (bool, optional): Flag to indicate if attention is causal.\n",
    "            update (str, optional): Update strategy for memory, either 'linear' or another strategy.\n",
    "        \"\"\"\n",
    "        super().__init__(config, is_cross_attention, layer_idx)\n",
    "\n",
    "        # Initializing memory state for compressive memory\n",
    "        self.d_head = config.hidden_size // config.num_attention_heads\n",
    "        self.n_head = config.num_attention_heads\n",
    "\n",
    "        # Initialize the beta parameter for combining A_mem and A_dot\n",
    "        self.beta = nn.Parameter(torch.zeros(1), requires_grad=True)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "\n",
    "        # Sequence length\n",
    "        self.seq_len = config.n_positions\n",
    "\n",
    "        self.is_causal = is_causal\n",
    "        self.register_buffer(\n",
    "            \"causal\",\n",
    "            torch.tril(\n",
    "                torch.ones(self.seq_len // n_segments, self.seq_len // n_segments)\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Segment size\n",
    "        self.n_segments = n_segments\n",
    "        self.segment_size = self.seq_len // n_segments\n",
    "\n",
    "        # Update strategy\n",
    "        self.update = update\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.FloatTensor,\n",
    "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
    "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "    ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n",
    "\n",
    "        batch_size, _, _ = hidden_states.size()\n",
    "\n",
    "        device = hidden_states.device\n",
    "\n",
    "        memory = torch.zeros((self.n_head, self.d_head, self.d_head), device=device)\n",
    "        z = torch.zeros((self.n_head, self.d_head, 1), device=device)\n",
    "\n",
    "        # Project hidden states to query, key, value\n",
    "        query, key, value = self.c_attn(hidden_states).split(self.split_size, dim=2)\n",
    "\n",
    "        # Reshape query, key, value for segmented processing\n",
    "        query = query.reshape(\n",
    "            batch_size,\n",
    "            self.n_head,\n",
    "            self.n_segments,\n",
    "            self.segment_size,\n",
    "            self.d_head,\n",
    "        )\n",
    "        key = key.reshape(\n",
    "            batch_size,\n",
    "            self.n_head,\n",
    "            self.n_segments,\n",
    "            self.segment_size,\n",
    "            self.d_head,\n",
    "        )\n",
    "        value = value.reshape(\n",
    "            batch_size,\n",
    "            self.n_head,\n",
    "            self.n_segments,\n",
    "            self.segment_size,\n",
    "            self.d_head,\n",
    "        )\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for idx in range(self.n_segments):\n",
    "\n",
    "            attention_mask_segment = attention_mask[\n",
    "                :, idx * self.segment_size : (idx + 1) * self.segment_size\n",
    "            ]\n",
    "\n",
    "            sigma_q = (\n",
    "                self.elu(query[:, :, idx, :, :]) + 1.0\n",
    "            )  # [bsz, n_head, segment, seq_len, head_dim]\n",
    "            sigma_k = (\n",
    "                self.elu(key[:, :, idx, :, :]) + 1.0\n",
    "            )  # [bsz, n_head, segment, seq_len, head_dim]\n",
    "\n",
    "            A_mem = (sigma_q @ memory) / (\n",
    "                (sigma_q @ z) + 1e-6\n",
    "            )  # [bsz, n_head, segment, seq_len, head_dim]\n",
    "\n",
    "            A_dot = query[:, :, idx, :, :] @ key[:, :, idx, :, :].transpose(-2, -1)\n",
    "\n",
    "            if self.is_causal:\n",
    "                A_dot.masked_fill_(self.causal == 0, float(\"-inf\"))\n",
    "\n",
    "            A_dot = F.softmax(\n",
    "                A_dot / torch.tensor(self.d_head, device=device) ** 0.5, dim=-1\n",
    "            )\n",
    "            A_dot = A_dot @ value[:, :, idx, :, :]\n",
    "\n",
    "            attention = (F.sigmoid(self.beta) * A_mem) + (\n",
    "                (1 - F.sigmoid(self.beta)) * A_dot\n",
    "            )\n",
    "\n",
    "            # Update memory\n",
    "            if self.update == \"linear\":\n",
    "                memory = memory + (sigma_k.transpose(-2, -1) @ value[:, :, idx, :, :])\n",
    "            else:\n",
    "                delta = (sigma_k @ memory) / ((sigma_k @ z) + 1e-6)\n",
    "                memory = memory + (\n",
    "                    sigma_k.transpose(-2, -1) @ (value[:, :, idx, :, :] - delta)\n",
    "                )\n",
    "\n",
    "            z = z + sigma_k.sum(dim=-2, keepdim=True)\n",
    "\n",
    "            outputs.append(attention)\n",
    "\n",
    "        # Concatenate outputs from all segments\n",
    "        final_output = torch.cat(outputs, dim=2).view(\n",
    "            batch_size, self.seq_len, self.embed_dim\n",
    "        )\n",
    "\n",
    "        return (final_output, None)\n",
    "\n",
    "\n",
    "# Supondo que o `config`, `hidden_states`, e `attention_mask` já tenham sido definidos.\n",
    "config = GPT2Config()\n",
    "config.n_positions = 1024  # Definindo um exemplo de tamanho de sequência\n",
    "hidden_states = torch.randn(2, 1024, config.hidden_size)  # Exemplo de estados ocultos\n",
    "attention_mask = torch.ones(2, 1024)  # Máscara de atenção de exemplo\n",
    "\n",
    "infini_att_gpt2 = InfiniAttentionGPT2(config=config, is_causal=True)\n",
    "\n",
    "# Forward\n",
    "outputs = infini_att_gpt2(hidden_states=hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "# Output\n",
    "print(\"Output InfiniAttention GPT-2 att:\")\n",
    "print(f\"{outputs[0].shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"gpt2\"  # \"gpt2\" or \"gpt2-infini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "dataset = [\"Este é um carro azul.\", \"Esta é uma casa vermelha.\"] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "if model_type == \"gpt2\":\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(dataset, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    model = GPT2LMHeadModel(config).to(device)\n",
    "else:\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\", padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    inputs = tokenizer(\n",
    "        dataset, padding=\"max_length\", truncation=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    model = GPT2LMHeadModel(config)\n",
    "\n",
    "    # Replace the attention module with InfiniAttention\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        model.transformer.h[i].attn = InfiniAttentionGPT2(\n",
    "            config, layer_idx=i, is_causal=False\n",
    "        )\n",
    "\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_dict(inputs)\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386b5ff7c18543c0bbbc963c4c14e5c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1/1:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 4.2348\n",
      "Perplexity: 69.0465\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epoch = 1\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "for epoch_idx in range(epoch):\n",
    "    print(f\"Epoch: {epoch_idx+1}/{epoch}\")\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(\n",
    "        train_loader, desc=f\"Training Epoch {epoch_idx+1}/{epoch}\", leave=False\n",
    "    ):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Acumula a perda para monitoramento\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Calcula a perda média da época\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f\"Loss: {avg_epoch_loss:.4f}\")\n",
    "    print(f\"Perplexity: {torch.exp(torch.tensor(avg_epoch_loss)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Este é um carro azul.\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "text = \"Este é um carro\"\n",
    "\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
    "\n",
    "output = model.generate(\n",
    "    input_ids, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "for out in output:\n",
    "    print(tokenizer.decode(out, skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
